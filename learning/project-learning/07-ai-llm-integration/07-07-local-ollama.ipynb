{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07-07: Ollama 本地模型部署",
    "",
    "使用 Ollama 在本地运行大语言模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Ollama 提供与 OpenAI 兼容的 API\n",
    "// 基础 URL: http://localhost:11434\n",
    "",
    "const OLLAMA_BASE_URL = 'http://localhost:11434';\n",
    "",
    "// ========== 1. 列出本地模型 ==========\n",
    "async function listModels() {\n",
    "  const response = await fetch(`${OLLAMA_BASE_URL}/api/tags`);\n",
    "  const data = await response.json();\n",
    "  return data.models;\n",
    "}\n",
    "",
    "// 返回示例:\n",
    "// [\n",
    "//   { name: 'llama3.2:latest', model: 'llama3.2:latest', ... },\n",
    "//   { name: 'codellama:latest', model: 'codellama:latest', ... },\n",
    "//   { name: 'mistral:latest', model: 'mistral:latest', ... }\n",
    "// ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// ========== 2. 生成文本 ==========\n",
    "async function generateText(prompt: string, model: string = 'llama3.2') {\n",
    "  const response = await fetch(`${OLLAMA_BASE_URL}/api/generate`, {\n",
    "    method: 'POST',\n",
    "    headers: { 'Content-Type': 'application/json' },\n",
    "    body: JSON.stringify({\n",
    "      model,\n",
    "      prompt,\n",
    "      stream: false\n",
    "    })\n",
    "  });\n",
    "  \n",
    "  const data = await response.json();\n",
    "  return data.response;\n",
    "}\n",
    "",
    "// 使用示例\n",
    "// const result = await generateText('Explain quantum computing in simple terms');\n",
    "// console.log(result);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// ========== 3. 流式生成 ==========\n",
    "async function* generateStream(prompt: string, model: string = 'llama3.2') {\n",
    "  const response = await fetch(`${OLLAMA_BASE_URL}/api/generate`, {\n",
    "    method: 'POST',\n",
    "    headers: { 'Content-Type': 'application/json' },\n",
    "    body: JSON.stringify({\n",
    "      model,\n",
    "      prompt,\n",
    "      stream: true\n",
    "    })\n",
    "  });\n",
    "  \n",
    "  const reader = response.body!.getReader();\n",
    "  const decoder = new TextDecoder();\n",
    "  \n",
    "  while (true) {\n",
    "    const { done, value } = await reader.read();\n",
    "    if (done) break;\n",
    "    \n",
    "    const chunk = decoder.decode(value);\n",
    "    const lines = chunk.split('\\n').filter(line => line.trim());\n",
    "    \n",
    "    for (const line of lines) {\n",
    "      try {\n",
    "        const data = JSON.parse(line);\n",
    "        if (data.response) {\n",
    "          yield data.response;\n",
    "        }\n",
    "        if (data.done) break;\n",
    "      } catch {\n",
    "        // Ignore invalid JSON\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "",
    "// 使用示例\n",
    "// for await (const chunk of generateStream('Write a poem about coding')) {\n",
    "//   process.stdout.write(chunk);\n",
    "// }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// ========== 4. 聊天对话 ==========\n",
    "interface ChatMessage {\n",
    "  role: 'system' | 'user' | 'assistant';\n",
    "  content: string;\n",
    "}\n",
    "",
    "async function chat(messages: ChatMessage[], model: string = 'llama3.2') {\n",
    "  const response = await fetch(`${OLLAMA_BASE_URL}/api/chat`, {\n",
    "    method: 'POST',\n",
    "    headers: { 'Content-Type': 'application/json' },\n",
    "    body: JSON.stringify({\n",
    "      model,\n",
    "      messages,\n",
    "      stream: false\n",
    "    })\n",
    "  });\n",
    "  \n",
    "  const data = await response.json();\n",
    "  return data.message;\n",
    "}\n",
    "",
    "// 使用示例\n",
    "// const response = await chat([\n",
    "//   { role: 'system', content: 'You are a helpful assistant.' },\n",
    "//   { role: 'user', content: 'Hello!' }\n",
    "// ]);\n",
    "// console.log(response.content);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// ========== 5. 拉取模型 ==========\n",
    "async function pullModel(model: string) {\n",
    "  const response = await fetch(`${OLLAMA_BASE_URL}/api/pull`, {\n",
    "    method: 'POST',\n",
    "    headers: { 'Content-Type': 'application/json' },\n",
    "    body: JSON.stringify({\n",
    "      name: model,\n",
    "      stream: false\n",
    "    })\n",
    "  });\n",
    "  \n",
    "  return await response.json();\n",
    "}\n",
    "",
    "// 删除模型\n",
    "async function deleteModel(model: string) {\n",
    "  const response = await fetch(`${OLLAMA_BASE_URL}/api/delete`, {\n",
    "    method: 'DELETE',\n",
    "    headers: { 'Content-Type': 'application/json' },\n",
    "    body: JSON.stringify({ name: model })\n",
    "  });\n",
    "  \n",
    "  return response.ok;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// ========== 6. 嵌入向量 ==========\n",
    "async function generateEmbedding(text: string, model: string = 'nomic-embed-text') {\n",
    "  const response = await fetch(`${OLLAMA_BASE_URL}/api/embeddings`, {\n",
    "    method: 'POST',\n",
    "    headers: { 'Content-Type': 'application/json' },\n",
    "    body: JSON.stringify({\n",
    "      model,\n",
    "      prompt: text\n",
    "    })\n",
    "  });\n",
    "  \n",
    "  const data = await response.json();\n",
    "  return data.embedding; // number[]\n",
    "}\n",
    "",
    "// 使用示例\n",
    "// const embedding = await generateEmbedding('Hello world');\n",
    "// console.log('Embedding size:', embedding.length); // 通常是 768 维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// ========== 7. OpenAI 兼容 API ==========\n",
    "// Ollama 提供 OpenAI 兼容的端点\n",
    "",
    "async function openAICompatibleChat() {\n",
    "  const response = await fetch(`${OLLAMA_BASE_URL}/v1/chat/completions`, {\n",
    "    method: 'POST',\n",
    "    headers: { 'Content-Type': 'application/json' },\n",
    "    body: JSON.stringify({\n",
    "      model: 'llama3.2',\n",
    "      messages: [\n",
    "        { role: 'user', content: 'Say hello' }\n",
    "      ]\n",
    "    })\n",
    "  });\n",
    "  \n",
    "  const data = await response.json();\n",
    "  return data.choices[0].message.content;\n",
    "}\n",
    "",
    "// 可以直接使用 OpenAI SDK 并修改 baseURL\n",
    "// import OpenAI from 'openai';\n",
    "// const client = new OpenAI({\n",
    "//   baseURL: 'http://localhost:11434/v1',\n",
    "//   apiKey: 'ollama' // required but ignored\n",
    "// });"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "name": "typescript"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}